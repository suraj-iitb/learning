{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99bdd39a",
   "metadata": {},
   "source": [
    "## 1. Human Translation\n",
    "**How can you translate the following sentence (taken from Europal dataset) into Hindi?**\n",
    "\n",
    "\"It has been of such depth that we can imagine how, for example, Ireland has made such extremely difficult adjustments, not because the International Money Fund says so, or because it has been imposed by anyone from Brussels, but because the Irish authorities consider it to be the best way to adjust its economy as soon as possible and move forward with the same impetus that it had before the crisis.\"\n",
    "\n",
    "**Answer 1**:\n",
    "1. Get an overall idea of what it is trying to convey by reading the sentence\n",
    "2. Start writing the translation without going back to the source sentence\n",
    "\n",
    "**Answer 2**:\n",
    "1. Get an overall idea of what it is trying to convey by reading the sentence\n",
    "2. Start writing the translation and while doing so, **revisit the different part of the source sentence** to ensure:\n",
    "    1. Our translation covers the entire sentence\n",
    "    2. Our translation ensures it is in equivalent tense\n",
    "    \n",
    "**Tip**: Try to mimic the same process in machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ed6c5",
   "metadata": {},
   "source": [
    "## 2. Machine Translation (Neural)\n",
    "\n",
    "### 2.1 Input, Output, & Problem Category\n",
    "**Input**: English sentence\n",
    "\n",
    "**Output**: Hindi sentence\n",
    "\n",
    "**Problem Category**: Sequence-sequence transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960e926",
   "metadata": {},
   "source": [
    "### 2.2 Architecture 1: Without Attention\n",
    "\n",
    "The following figure shows architecture for neural machine translation:\n",
    "\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/enc_dec.jpeg\" style=\"width:80%;\">\n",
    "  <img src=\"./assets/images/enc_dec_unrolled.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">Architecture for neural machine translation</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### 2.2.1 Architecture 1 Internals\n",
    "It follows encoder-decoder paradigm.\n",
    "\n",
    "**Encoder**\n",
    "It consists of following layers:\n",
    "1. Embedding layer\n",
    "2. Recurrent layer\n",
    "\n",
    "*Input to recurrent layer*:\n",
    "1. Embedding for the ith word at ith timestamp\n",
    "\n",
    "**Decoder**\n",
    "It consists of following layers:\n",
    "1. Embedding layer\n",
    "2. Recurrent layer\n",
    "3. Softmax layer\n",
    "\n",
    "*Input to recurrent layer*:\n",
    "1. Hidden state from the last timestep of the encoder -> **input** at every timestep of the decoder\n",
    "2. Embedding for the produced word from the last timestep of the decoder -> **input** at current timestep of the decoder\n",
    "    1.  At timestep zero, there is no produced word: So we will use <START> embedding\n",
    "3. The above two inputs at every timestep of the decoder are concatenated and then fed\n",
    "    \n",
    "### 2.2.2 Architecture 1 Downsides\n",
    "    1. Does not work well for long sentences since it is encoding the long sentence in a fixed-size vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c743a1e4",
   "metadata": {},
   "source": [
    "## 2.3 Architecture 2: With Attention\n",
    "\n",
    "The following figure shows architecture for neural machine translation:\n",
    "\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/enc_dec_with_attn.jpeg\" style=\"width:80%;\">\n",
    "  <img src=\"./assets/images/enc_dec_with_attn_unrolled.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">Architecture for neural machine translation</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### 2.3.1 Architecture 1 Internals\n",
    "It follows encoder-decoder paradigm.\n",
    "\n",
    "**Encoder**\n",
    "It consists of following layers:\n",
    "1. Embedding layer\n",
    "2. Recurrent layer\n",
    "\n",
    "*Input to recurrent layer*:\n",
    "1. Embedding for the ith word at ith timestamp\n",
    "\n",
    "*Change*: \n",
    "1. Store the hidden state at every timestep (we will make use of it during decoding) \n",
    "\n",
    "**Decoder**\n",
    "It consists of following layers:\n",
    "1. Embedding layer\n",
    "2. Recurrent layer\n",
    "3. Softmax layer\n",
    "\n",
    "*Input to recurrent layer*:\n",
    "1. Hidden state from the last timestep of the encoder -> **input** at every timestep of the decoder (**Instead of hidden state from the last timestep of the encoder we will feed the resulting vector of the encoder which we will discuss**)\n",
    "2. Embedding for the produced word from the last timestep of the decoder -> **input** at current timestep of the decoder\n",
    "    1.  At timestep zero, there is no produced word: So we will use <START> embedding\n",
    "3. The above two inputs at every timestep of the decoder are concatenated and then fed\n",
    "    \n",
    "*Change*\n",
    "1. Compute the aligment score for each source hidden state vector at each timestep\n",
    "    1. Significance of alignment score: How much attention to pay to that source hidden state vector at a each timestep\n",
    "    2. How we will calculate it: We will discuss it\n",
    "2. Normalize the aligment scores at each timestep \n",
    "3. Calculate weighted sum at each timestep\n",
    "    1. Multiply each source hidden state vector by its alignment score\n",
    "    2. Add the resulting vectors\n",
    "4. The resulting vector is fed to decoder at each timestep\n",
    "    1. This vector is different at each timestep (t) as alignment scores are controlled by the internal state of the decoder at timestep (t-1)\n",
    "\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/align_score.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">How encoder internal state is combined with alignment vectors to create decoder input for each timestep</figcaption>\n",
    "</figure>\n",
    "    \n",
    "**Tip**:\n",
    "1. When we try to generate *I* at first timestep, the focus will be on *je*\n",
    "2. When we try to generate *am* at second timestep, the focus will be on *suis*\n",
    "3. When we try to generate *a* at the third timestep, the focus will on *suis* and *etudiant*\n",
    "4. When we try to generate *student* at fourth timestep, the focus will be on *etudiant*\n",
    "    \n",
    "### 2.3.2 Computing the alignment scores\n",
    "1. A vector of alignment scores is called alignment vector\n",
    "    1. An alignment vector consists of T<sub>e</sub> elements (Where T<sub>e</sub> is number of timesteps for the encoder)\n",
    "    2. We need to calculate T<sub>d</sub> such vectors (Where T<sub>d</sub> is number of timesteps for the decoder)\n",
    "2. Some decisions which could be used to calculate the alignment scores:\n",
    "    1. What **input values** to use to compute the vector: decoder hidden state, source hidden state (**very strange as we are using source hidden states to find the alignment scores for source hidden states**)\n",
    "    2. What **computations** to apply to these input values?\n",
    "\n",
    "**Mathematical Formulations**:\n",
    "    \n",
    "*Query*: target hidden state / decoder hidden state\n",
    "    \n",
    "*Key*: source hidden state / encoder hidden state\n",
    "    \n",
    "*Value*: source hidden state (it can be different from key in other problems)\n",
    "    \n",
    "*Problem*: Find a function which matches the query to the key (we can assume this function to be a feed-forward neural network and let the model learn the function itself)\n",
    "\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/ffnn_for_align_score.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">Function as FFNN</figcaption>\n",
    "</figure>\n",
    "    \n",
    "**Left side FFNN**:\n",
    "1. Structure:\n",
    "    1. Arbitrary no. of fully connected layers\n",
    "    2. 1 fully connected softmax layer\n",
    "2. Drawbacks:\n",
    "    1. Hardcodes expected position of words in source sentence\n",
    "    2. Restrictions on source sentence length\n",
    "    \n",
    "**Right side FFNN**:\n",
    "1. Structure:\n",
    "    1. 1 fully connected layers (with tanh)\n",
    "    2. 1 fully connected layer (without activation function)\n",
    "    3. 1 softmax layer\n",
    "2. Multiple instances of this structure with weight sharing\n",
    "3. Equations for the FFNN is shown below:\n",
    "\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/score_eq1.jpeg\" style=\"width:80%\">\n",
    "  <img src=\"./assets/images/score_eq2.jpeg\" style=\"width:80%\">\n",
    "  <img src=\"./assets/images/score_eq3.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">Equations for FFNN</figcaption>\n",
    "</figure>\n",
    "    \n",
    "4. **Tip**: There can be simpler scoring functions as shown below.\n",
    "    1. Dot product version combined with the softmax function represents right side FFNN but with the modifications that there is no fully connected layer before the softmax layer and the neurons in the softmax layer use the target hidden state vector as neuron weights.\n",
    "    2. General version combined with the softmax function represents right side FFNN but with modifications that first layer have linear activation function along with input as source hidden state only and the neurons in the softmax layer use the target hidden state vector as neuron weights.\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/other_scoring.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">Other scoring functions</figcaption>\n",
    "</figure>\n",
    " \n",
    "### 2.3.3 Soft vs Hard Attention\n",
    "**Hard Attention**: Single encoder hidden state is selected to focus on each decoder timestep\n",
    "\n",
    "**Soft Attention**: \n",
    "1. A mixture (weighted sum) of encoder hidden states is used on each decoder timestep\n",
    "2. Why soft attention: function is continuous and differentiable thus enable the use of backpropagation\n",
    "    \n",
    "    \n",
    "### 2.3.4 Architecture 1 Downsides\n",
    "    1. Does not work well for long sentences as we now need more space to store encoder hidden state vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db87ae",
   "metadata": {},
   "source": [
    "## 2.4 Architecture 3: Transformer\n",
    "\n",
    "**Why transformers**:\n",
    "1. RNN (with or without attention) does not work well for longer sentences\n",
    "2. RNN is inherently serial (transformer mainly focuses on this problem solution)\n",
    "\n",
    "### 2.4.1 Self Attention\n",
    "**Decoder Attention**: Focus on different parts of **source hidden states** by a target hidden state\n",
    "\n",
    "**Self Anntention**: Focus on differents parts of **preceding layer outputs** by a current word position\n",
    "\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/self_attn.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">Embedding layer -> Self-attention layer -> Fully connected layer</figcaption>\n",
    "</figure>\n",
    "\n",
    "*Query*: Previous layer current word\n",
    "\n",
    "*Key*: Previous layer all word\n",
    "\n",
    "*Value*: Previous layer all word\n",
    "\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/self_attn_eq.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">Attention calculation (attention score, normalization & weighted sum)</figcaption>\n",
    "</figure>\n",
    "\n",
    "**Tip**: Attention mechanism pass the above inputs through a 3 seperate single layer FFNN with linear activation functions to obtain query, key, and value.\n",
    "\n",
    "Why: \n",
    "1. Different widths of query, key, value than original input vector. Thus different width of output vector (Thats the requirment of seq-seq transformation)\n",
    "2. Key is different than the value (More generalization of attention)\n",
    "\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/self_attn_proj.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">Attention mechanism with projection layers that modify the dimensions of the quer, key, and value</figcaption>\n",
    "</figure>\n",
    "\n",
    "### 2.4.2 Multi Head Attention\n",
    "1. Multiple attention mechanism operates in parallel for each input vector\n",
    "2. The outputs of these multiple heads are concatenated and feed to the projection layer (so that we get the desired dimension of the output vector)\n",
    "\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/multi_head_attn.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">Multi-head attention mechanism</figcaption>\n",
    "</figure>\n",
    "\n",
    "### 2.4.3 Transformer\n",
    "Transformer follows encoder-deccoder paradigm\n",
    "\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/transformer.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">Transformer Modules</figcaption>\n",
    "</figure>\n",
    "\n",
    "#### 2.4.3.1 Transformer Internals\n",
    "\n",
    "**Encoder**:\n",
    "1. Embedding layer\n",
    "2. 6 identical modules where each module consists of:\n",
    "    1. Multi-head self-attention\n",
    "    2. Skip connection & layer normalization\n",
    "    3. FFNN (2 layers)\n",
    "    4. Skip connection & layer normalization\n",
    "    \n",
    "**Decoder (Autoregressive)**:\n",
    "1. Multi-head self-attention (with masking to prevent attending to future words)\n",
    "2. Skip connection & layer normalization\n",
    "3. Multi-head traditional attention\n",
    "4. Skip connection & layer normalization\n",
    "5. FFNN (2 layers)\n",
    "6. Skip connection & layer normalization\n",
    "\n",
    "The full transformer architecture is shown below:\n",
    "\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/full_transformer.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">Full transformer architecture</figcaption>\n",
    "</figure>\n",
    "\n",
    "**Positional Encoding**:\n",
    "\n",
    "1. How to take word order into consideration: positional encoding\n",
    "2. What is the dimesnion of positional encoding: same as input embedding\n",
    "\n",
    "<figure>\n",
    "  <br>\n",
    "  <img src=\"./assets/images/pos_enc.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\">Positional encoding</figcaption>\n",
    "</figure>\n",
    "\n",
    "3. How to calculate the positional embedding\n",
    "    1. sine & cosine functions of different frequencies\n",
    "    2. Learn positional encoding\n",
    "\n",
    "### 2.4.4 How parallelism is achieved\n",
    "1. There is no dependency within a single layer\n",
    "2. Each instance of embedding, query, key, value, self-attention, fully connected layers share weights (in a particular head)\n",
    "3. Heads are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb16e2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
